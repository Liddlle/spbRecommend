---
title: "lda_texts"
output: html_document
---

```{r all datasets}
events=read.csv("~/spbRecommend/events.csv")
places=read.csv("~/spbRecommend/places.csv")

likes=read.csv("/students/aabakhitova/spb_files/likes_vk2000.csv")
posts=read.csv("/students/aabakhitova/spb_files/vk_posts_light.csv")
```
```{r wordcloud places}
library(wordcloud)

places$tags = gsub("'", "", places$tags)
places$tags = gsub("\\[|\\]", " ", places$tags)
places$tags = as.character(places$tags)
places$tags <- removeWords(places$tags, stopwords("english"))
wCorpus12 <- Corpus(VectorSource(places$tags))
wCorpus12 <- tm_map(wCorpus12, content_transformer(tolower))
wCorpus12 <- tm_map(wCorpus12, removePunctuation)
wCorpus12 <- tm_map(wCorpus12, stemDocument)

tdm.data <- TermDocumentMatrix(wCorpus12)
words_matrix12 <- as.matrix(tdm.data)
colnames(words_matrix12) <- places$tags

words_freq12 <- sort(rowSums(words_matrix12), decreasing=TRUE)
words_freq12 <- data.frame(freq = words_freq12, word = names(words_freq12))
rownames(words_freq12) <- NULL 

wordcloud(words = words_freq12$word, freq = words_freq12$freq, scale=c(3,.1), min.freq = 50,
max.words=Inf, random.order=FALSE, rot.per=0.1,
ordered.colors=FALSE, colors=brewer.pal(8, "Dark2"))

```
```{r wordcloud events}

events$tags = gsub("'", "", events$tags)
events$tags = gsub("\\[|\\]", " ", events$tags)
events$tags = as.character(events$tags)
events$tags <- removeWords(events$tags, stopwords("english"))
wCorpus12 <- Corpus(VectorSource(events$tags))
wCorpus12 <- tm_map(wCorpus12, content_transformer(tolower))
wCorpus12 <- tm_map(wCorpus12, removePunctuation)
wCorpus12 <- tm_map(wCorpus12, stemDocument)

tdm.data <- TermDocumentMatrix(wCorpus12)
words_matrix12 <- as.matrix(tdm.data)
colnames(words_matrix12) <- events$tags

words_freq12 <- sort(rowSums(words_matrix12), decreasing=TRUE)
words_freq12 <- data.frame(freq = words_freq12, word = names(words_freq12))
rownames(words_freq12) <- NULL 

wordcloud(words = words_freq12$word, freq = words_freq12$freq, scale=c(3,.1), min.freq = 50,
max.words=Inf, random.order=FALSE, rot.per=0.1,
ordered.colors=FALSE, colors=brewer.pal(8, "Dark2"))
```

```{r lda on posts}
library(mallet)
library(dplyr)
library(stringr)

posts$id=as.character(posts$id)
posts$text=as.character(posts$text)

# токенизация текстов
mallet.instances <- mallet.import(posts$id, posts$text, "stopwords.txt", token.regexp = "[\\p{L}\\p{N}-]*\\p{L}+")

## настраиваем параметры модели и загружаем данные
topic.model <- MalletLDA(num.topics=100) # количество тем
topic.model$loadDocuments(mallet.instances) 
topic.model$setAlphaOptimization(20, 50) # оптимизация гиперпараметров

## собираем статистику: словарь и частотность
vocabulary <- topic.model$getVocabulary() # словарь корпуса
word.freqs <- mallet.word.freqs(topic.model) # таблица частотности слов

## параметр — количество итераций
topic.model$train(1000)

## выбор наилучшей темы для каждого токена
topic.model$maximize(10)

## таблица распределения тем по документам
doc.topics <- mallet.doc.topics(topic.model, smoothed=TRUE, normalized=TRUE)
## таблица распределения слов по темам
topic.words <- mallet.topic.words(topic.model, smoothed=TRUE, normalized=TRUE)

## просмотр топ-10 слов для всех тем
df = data.frame()
for (k in 1:nrow(topic.words)) {
  top <- paste(mallet.top.words(topic.model, topic.words[k,], 20)$words,collapse=" ")
  df = rbind(df, data.frame(k, top))
}
write.csv(df,"categores_posts.csv")
```

```{r lda on events}
events$id=as.character(events$id)
events$description=as.character(events$description)

# токенизация текстов
mallet.instances <- mallet.import(events$id, events$description, "stopwords.txt", token.regexp = "[\\p{L}\\p{N}-]*\\p{L}+")

## настраиваем параметры модели и загружаем данные
topic.model <- MalletLDA(num.topics=100) # количество тем
topic.model$loadDocuments(mallet.instances) 
topic.model$setAlphaOptimization(20, 50) # оптимизация гиперпараметров

## собираем статистику: словарь и частотность
vocabulary <- topic.model$getVocabulary() # словарь корпуса
word.freqs <- mallet.word.freqs(topic.model) # таблица частотности слов

## параметр — количество итераций
topic.model$train(1000)

## выбор наилучшей темы для каждого токена
topic.model$maximize(10)

## таблица распределения тем по документам
doc.topics <- mallet.doc.topics(topic.model, smoothed=TRUE, normalized=TRUE)
## таблица распределения слов по темам
topic.words <- mallet.topic.words(topic.model, smoothed=TRUE, normalized=TRUE)

## просмотр топ-10 слов для всех тем
df = data.frame()
for (k in 1:nrow(topic.words)) {
  top <- paste(mallet.top.words(topic.model, topic.words[k,], 20)$words,collapse=" ")
  df = rbind(df, data.frame(k, top))
}
write.csv(df,"categores_events.csv")
```

```{r lda on places}
places$id=as.character(places$id)
places$description=as.character(places$description)

# токенизация текстов
mallet.instances <- mallet.import(places$id, places$description, "stopwords.txt", token.regexp = "[\\p{L}\\p{N}-]*\\p{L}+")

## настраиваем параметры модели и загружаем данные
topic.model <- MalletLDA(num.topics=100) # количество тем
topic.model$loadDocuments(mallet.instances) 
topic.model$setAlphaOptimization(20, 50) # оптимизация гиперпараметров

## собираем статистику: словарь и частотность
vocabulary <- topic.model$getVocabulary() # словарь корпуса
word.freqs <- mallet.word.freqs(topic.model) # таблица частотности слов

## параметр — количество итераций
topic.model$train(1000)

## выбор наилучшей темы для каждого токена
topic.model$maximize(10)

## таблица распределения тем по документам
doc.topics <- mallet.doc.topics(topic.model, smoothed=TRUE, normalized=TRUE)
## таблица распределения слов по темам
topic.words <- mallet.topic.words(topic.model, smoothed=TRUE, normalized=TRUE)

## просмотр топ-10 слов для всех тем
df = data.frame()
for (k in 1:nrow(topic.words)) {
  top <- paste(mallet.top.words(topic.model, topic.words[k,], 20)$words,collapse=" ")
  df = rbind(df, data.frame(k, top))
}
write.csv(df,"categores_places.csv")
```

```{r map}
library(ggplot2)
library(ggmap)
library(tidyr)

# places<-separate(places, col=subway, into=c("subway1", "subway2","subway3"), sep=",")
places=inner_join(places,categores_places, by="topic")

map = get_map(location = c("санкт-петербург"), zoom = 11, color = "bw", maptype = "roadmap")

ggmap(map) +
  geom_point(data = places, aes(x = lon, y = lat, color=category), alpha = 0.5, size = 0.7)

```

```{r some other stuff}
doctop=data.frame(doc.topics)
places$topic=colnames(doctop)[max.col(doctop,ties.method="first")]
write.csv(places,"places.csv")
```
```{r categories}
library(tm)
events$categories = gsub("'", "", events$categories)
events$categories = gsub("\\[|\\]", " ", events$categories)
events$categories = as.character(events$categories)
events$categories <- removeWords(events$categories, stopwords("english"))
wCorpus12 <- Corpus(VectorSource(events$categories))
wCorpus12 <- tm_map(wCorpus12, content_transformer(tolower))
wCorpus12 <- tm_map(wCorpus12, removePunctuation)
wCorpus12 <- tm_map(wCorpus12, stemDocument)

tdm.data <- TermDocumentMatrix(wCorpus12)
words_matrix12 <- as.matrix(tdm.data)
colnames(words_matrix12) <- events$categories

words_freq12 <- sort(rowSums(words_matrix12), decreasing=TRUE)
words_freq12 <- data.frame(freq = words_freq12, word = names(words_freq12))
rownames(words_freq12) <- NULL  
head(words_freq12)

write.csv(words_freq12, "catfreq.csv")
words=head(words_freq12,10)%>%dplyr::arrange(desc(freq))
ggplot()+geom_bar(data=words,aes(x=reorder(word,desc(freq)),y=freq), fill="darkolivegreen2",stat="identity")+theme(axis.text.x = element_text(angle = 45, hjust = 1))+theme(axis.title.x = element_text(size=18), axis.title.y = element_text(size=16),legend.text=element_text(size=12), axis.text.y=element_text(size=11), axis.text.x=element_text(size=11)) +xlab("Категория")+ylab("Частотность")
```

```{r cluster visualization}
only_year$age=2017-only_year$Year
median(only_year$age)
mean(only_year$age)


cluster_category_freq=cluster_category_freq%>%group_by(clust_5)%>%dplyr::arrange(desc(total_mentions))%>%top_n(10)
cluster_category_freq$clust_5=as.factor(cluster_category_freq$clust_5)
cluster_category_freq=ungroup(cluster_category_freq)
cluster_category_freq = cluster_category_freq %>% arrange(total_mentions) %>% 
  mutate(clust_5=ordered(clust_5, levels = unique(cluster_category_freq$clust_5)))

ggplot()+geom_bar(data=cluster_category_freq,aes(x=reorder(category,desc(total_mentions)),y=total_mentions,fill=clust_5),stat="identity")+ facet_grid(clust_5 ~ .)+theme(axis.text.x = element_text(angle = 45, hjust = 1))+theme(axis.title.x = element_text(size=18), axis.title.y = element_text(size=16),legend.text=element_text(size=12), axis.text.y=element_text(size=11), axis.text.x=element_text(size=11)) +xlab("Категория")+ylab("Частотность")

```