---
title: "lda_texts"
output: html_document
---

```{r all datasets}
events=read.csv("~/spbRecommend/events.csv")
places=read.csv("~/spbRecommend/places.csv")

likes=read.csv("/students/aabakhitova/spbRecommend/likes_vk2000.csv")
posts=read.csv("/students/aabakhitova/spbRecommend/vk_posts_light.csv")
```

```{r map}
library(ggplot2)
library(ggmap)
library(tidyr)

# places<-separate(places, col=subway, into=c("subway1", "subway2","subway3"), sep=",")
places=inner_join(places,categores_places, by="topic")

map = get_map(location = c("санкт-петербург"), zoom = 11, color = "bw", maptype = "roadmap")

ggmap(map) +
  geom_point(data = places, aes(x = lon, y = lat, color=category), alpha = 0.5, size = 0.7)
# позже придумаю что делать с категориями для цвета

```
```{r lda on posts}
library(mallet)
library(dplyr)
library(stringr)

posts$id=as.character(posts$id)
posts$text=as.character(posts$text)

# токенизация текстов
mallet.instances <- mallet.import(posts$id, posts$text, "stopwords.txt", token.regexp = "[\\p{L}\\p{N}-]*\\p{L}+")

## настраиваем параметры модели и загружаем данные
topic.model <- MalletLDA(num.topics=100) # количество тем
topic.model$loadDocuments(mallet.instances) 
topic.model$setAlphaOptimization(20, 50) # оптимизация гиперпараметров

## собираем статистику: словарь и частотность
vocabulary <- topic.model$getVocabulary() # словарь корпуса
word.freqs <- mallet.word.freqs(topic.model) # таблица частотности слов

## параметр — количество итераций
topic.model$train(1000)

## выбор наилучшей темы для каждого токена
topic.model$maximize(10)

## таблица распределения тем по документам
doc.topics <- mallet.doc.topics(topic.model, smoothed=TRUE, normalized=TRUE)
## таблица распределения слов по темам
topic.words <- mallet.topic.words(topic.model, smoothed=TRUE, normalized=TRUE)

## просмотр топ-10 слов для всех тем
df = data.frame()
for (k in 1:nrow(topic.words)) {
  top <- paste(mallet.top.words(topic.model, topic.words[k,], 20)$words,collapse=" ")
  df = rbind(df, data.frame(k, top))
}
write.csv(df,"categores_posts.csv")
```

```{r lda on events}
events$id=as.character(events$id)
events$description=as.character(events$description)

# токенизация текстов
mallet.instances <- mallet.import(events$id, events$description, "stopwords.txt", token.regexp = "[\\p{L}\\p{N}-]*\\p{L}+")

## настраиваем параметры модели и загружаем данные
topic.model <- MalletLDA(num.topics=100) # количество тем
topic.model$loadDocuments(mallet.instances) 
topic.model$setAlphaOptimization(20, 50) # оптимизация гиперпараметров

## собираем статистику: словарь и частотность
vocabulary <- topic.model$getVocabulary() # словарь корпуса
word.freqs <- mallet.word.freqs(topic.model) # таблица частотности слов

## параметр — количество итераций
topic.model$train(1000)

## выбор наилучшей темы для каждого токена
topic.model$maximize(10)

## таблица распределения тем по документам
doc.topics <- mallet.doc.topics(topic.model, smoothed=TRUE, normalized=TRUE)
## таблица распределения слов по темам
topic.words <- mallet.topic.words(topic.model, smoothed=TRUE, normalized=TRUE)

## просмотр топ-10 слов для всех тем
df = data.frame()
for (k in 1:nrow(topic.words)) {
  top <- paste(mallet.top.words(topic.model, topic.words[k,], 20)$words,collapse=" ")
  df = rbind(df, data.frame(k, top))
}
write.csv(df,"categores_events.csv")
```

```{r lda on places}
places$id=as.character(places$id)
places$description=as.character(places$description)

# токенизация текстов
mallet.instances <- mallet.import(places$id, places$description, "stopwords.txt", token.regexp = "[\\p{L}\\p{N}-]*\\p{L}+")

## настраиваем параметры модели и загружаем данные
topic.model <- MalletLDA(num.topics=100) # количество тем
topic.model$loadDocuments(mallet.instances) 
topic.model$setAlphaOptimization(20, 50) # оптимизация гиперпараметров

## собираем статистику: словарь и частотность
vocabulary <- topic.model$getVocabulary() # словарь корпуса
word.freqs <- mallet.word.freqs(topic.model) # таблица частотности слов

## параметр — количество итераций
topic.model$train(1000)

## выбор наилучшей темы для каждого токена
topic.model$maximize(10)

## таблица распределения тем по документам
doc.topics <- mallet.doc.topics(topic.model, smoothed=TRUE, normalized=TRUE)
## таблица распределения слов по темам
topic.words <- mallet.topic.words(topic.model, smoothed=TRUE, normalized=TRUE)

## просмотр топ-10 слов для всех тем
df = data.frame()
for (k in 1:nrow(topic.words)) {
  top <- paste(mallet.top.words(topic.model, topic.words[k,], 20)$words,collapse=" ")
  df = rbind(df, data.frame(k, top))
}
write.csv(df,"categores_places.csv")
```

```{r}
doctop=data.frame(doc.topics)
places$topic=colnames(doctop)[max.col(doctop,ties.method="first")]
write.csv(places,"places.csv")
```
```{r categories}
library(tm)
events$categories = gsub("'", "", events$categories)
events$categories = gsub("\\[|\\]", " ", events$categories)
events$categories = as.character(events$categories)
events$categories <- removeWords(events$categories, stopwords("english"))
wCorpus12 <- Corpus(VectorSource(events$categories))
wCorpus12 <- tm_map(wCorpus12, content_transformer(tolower))
wCorpus12 <- tm_map(wCorpus12, removePunctuation)
wCorpus12 <- tm_map(wCorpus12, stemDocument)

tdm.data <- TermDocumentMatrix(wCorpus12)
words_matrix12 <- as.matrix(tdm.data)
colnames(words_matrix12) <- events$categories

words_freq12 <- sort(rowSums(words_matrix12), decreasing=TRUE)
words_freq12 <- data.frame(freq = words_freq12, word = names(words_freq12))
rownames(words_freq12) <- NULL  
head(words_freq12)

write.csv(words_freq12, "catfreq.csv")
```