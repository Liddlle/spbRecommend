---
title: "lda_texts"
output: html_document
---

```{r all datasets}
events=read.csv("~/spbRecommend/events.csv")
places=read.csv("~/spbRecommend/places.csv")

likes=read.csv("/students/aabakhitova/spbRecommend/likes_vk2000.csv")
posts=read.csv("/students/aabakhitova/spbRecommend/vk_posts_light.csv")
```

```{r map}
library(ggplot2)
library(ggmap)

map = get_map(location = c("санкт-петербург"), zoom = 11, color = "bw", maptype = "roadmap")

ggmap(map) +
  geom_point(data = places, aes(x = lon, y = lat), alpha = 0.5, size = 0.7)
# позже придумаю что делать с категориями для цвета
```
```{r lda on posts}
library(mallet)
library(dplyr)
library(stringr)

posts$id=as.character(posts$id)
posts$text=as.character(posts$text)

# токенизация текстов
mallet.instances <- mallet.import(posts$id, posts$text, "stopwords.txt", token.regexp = "[\\p{L}\\p{N}-]*\\p{L}+")

## настраиваем параметры модели и загружаем данные
topic.model <- MalletLDA(num.topics=100) # количество тем
topic.model$loadDocuments(mallet.instances) 
topic.model$setAlphaOptimization(20, 50) # оптимизация гиперпараметров

## собираем статистику: словарь и частотность
vocabulary <- topic.model$getVocabulary() # словарь корпуса
word.freqs <- mallet.word.freqs(topic.model) # таблица частотности слов

## параметр — количество итераций
topic.model$train(1000)

## выбор наилучшей темы для каждого токена
topic.model$maximize(10)

## таблица распределения тем по документам
doc.topics <- mallet.doc.topics(topic.model, smoothed=TRUE, normalized=TRUE)
## таблица распределения слов по темам
topic.words <- mallet.topic.words(topic.model, smoothed=TRUE, normalized=TRUE)

## просмотр топ-10 слов для всех тем
df = data.frame()
for (k in 1:nrow(topic.words)) {
  top <- paste(mallet.top.words(topic.model, topic.words[k,], 20)$words,collapse=" ")
  df = rbind(df, data.frame(k, top))
}
write.csv(df,"categores_posts.csv")
```

```{r lda on events}
events$id=as.character(events$id)
events$description=as.character(events$description)

# токенизация текстов
mallet.instances <- mallet.import(events$id, events$description, "stopwords.txt", token.regexp = "[\\p{L}\\p{N}-]*\\p{L}+")

## настраиваем параметры модели и загружаем данные
topic.model <- MalletLDA(num.topics=100) # количество тем
topic.model$loadDocuments(mallet.instances) 
topic.model$setAlphaOptimization(20, 50) # оптимизация гиперпараметров

## собираем статистику: словарь и частотность
vocabulary <- topic.model$getVocabulary() # словарь корпуса
word.freqs <- mallet.word.freqs(topic.model) # таблица частотности слов

## параметр — количество итераций
topic.model$train(1000)

## выбор наилучшей темы для каждого токена
topic.model$maximize(10)

## таблица распределения тем по документам
doc.topics <- mallet.doc.topics(topic.model, smoothed=TRUE, normalized=TRUE)
## таблица распределения слов по темам
topic.words <- mallet.topic.words(topic.model, smoothed=TRUE, normalized=TRUE)

## просмотр топ-10 слов для всех тем
df = data.frame()
for (k in 1:nrow(topic.words)) {
  top <- paste(mallet.top.words(topic.model, topic.words[k,], 20)$words,collapse=" ")
  df = rbind(df, data.frame(k, top))
}
write.csv(df,"categores_events.csv")
```

```{r lda on places}
places$id=as.character(places$id)
places$description=as.character(places$description)

# токенизация текстов
mallet.instances <- mallet.import(places$id, places$description, "stopwords.txt", token.regexp = "[\\p{L}\\p{N}-]*\\p{L}+")

## настраиваем параметры модели и загружаем данные
topic.model <- MalletLDA(num.topics=100) # количество тем
topic.model$loadDocuments(mallet.instances) 
topic.model$setAlphaOptimization(20, 50) # оптимизация гиперпараметров

## собираем статистику: словарь и частотность
vocabulary <- topic.model$getVocabulary() # словарь корпуса
word.freqs <- mallet.word.freqs(topic.model) # таблица частотности слов

## параметр — количество итераций
topic.model$train(1000)

## выбор наилучшей темы для каждого токена
topic.model$maximize(10)

## таблица распределения тем по документам
doc.topics <- mallet.doc.topics(topic.model, smoothed=TRUE, normalized=TRUE)
## таблица распределения слов по темам
topic.words <- mallet.topic.words(topic.model, smoothed=TRUE, normalized=TRUE)

## просмотр топ-10 слов для всех тем
df = data.frame()
for (k in 1:nrow(topic.words)) {
  top <- paste(mallet.top.words(topic.model, topic.words[k,], 20)$words,collapse=" ")
  df = rbind(df, data.frame(k, top))
}
write.csv(df,"categores_places.csv")
```